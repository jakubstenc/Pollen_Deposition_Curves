# Pollen Deposition Curves Project

**Automated workflow for extracting, digitizing, and reporting pollen deposition data from scientific literature.**

This project manages the data lifecycle for the "Pollen Deposition Curves" study, ensuring that data extracted from literature figures is verifiable, reproducible, and easily accessible.

## ğŸ“š Documentation

The project includes a self-contained documentation website generated from the data.

ğŸ‘‰ **[View the Project Website & Report](doc/index.html)**
*(Open this link in your browser to explore the extracted data, visualizations, and workflow details.)*

## ğŸ”„ Workflow

The system follows a modular "Payload" architecture:

1.  **Digitization**: Raw data points are extracted from published graphs (using tools like WebPlotDigitizer or automated scripts).
2.  **Payload Creation**: Data is refined into a "flat table" structure and saved as Python scripts in `data/payloads/` (e.g., `evanhoe2002.py`).
3.  **Processing**: The shared module `process_data.py`:
    *   Validates the data structure.
    *   Generates interactive HTML tables.
    *   Reconstructs the original graphs side-by-side with source images for Quality Control (QC).
    *   **Exports CSVs** automatically to `data/csv/`.
4.  **Reporting**: Quarto renders the website (`doc/`), aggregating all payloads into a unified report.

## ğŸ› ï¸ Installation & Usage

### Prerequisites
*   Python 3.x
*   Quarto CLI
*   Standard Python data stack (`pandas`, `matplotlib`, `IPython`, `jupyter`)

### Rendering the Project
To generate the website (including the report and CSV exports), run the helper script:

```bash
./render.sh
```

This script ensures Quarto uses the project's local virtual environment.

## ğŸ“‚ Project Structure

*   `data/payloads/`: **Source of Truth**. contains Python files with extracted data for each article.
*   `data/csv/`: **Exported Data**. Clean CSV files generated automatically during rendering.
*   `doc/`: **Website Output**. The rendered HTML files (`index.html`, `report.html`, etc.).
*   `Data_images/`: Original source images organized by bibliography nickname.
*   `process_data.py`: Core logic for data validation, plotting, and export.
*   `report.qmd`: Quarto notebook that aggregates all payloads.
*   `workflow.qmd`: Detailed documentation of the digitization process.

## ğŸ“ Adding New Data

1.  Create a new payload file in `data/payloads/` (e.g., `authorYEAR.py`).
2.  Define the `flat_data_table` list containing your data points.
3.  Add a new section to `report.qmd`:
    ```python
    from data.payloads import authorYEAR
    process_payload(authorYEAR.flat_data_table)
    ```
4.  Run `./render.sh`.

---
*Generated by [Antigravity](https://github.com/google-deepmind/antigravity)*